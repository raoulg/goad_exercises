{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear data\n",
    "## 1.1 generating synthetic data\n",
    "Let's create some linear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_linear(\n",
    "    filename: Path, size: int = 100, a: float = 2, b: float = 4, s: float = 0.5\n",
    "):\n",
    "    if filename.exists():\n",
    "        logger.info(f\"Data already generated in {filename.absolute()}\")\n",
    "    else:\n",
    "        x = np.linspace(0, 1, size)\n",
    "        noise = np.random.normal(scale=s, size=len(x))\n",
    "        y = a * x + b + noise\n",
    "        pd.DataFrame({\"x\": x, \"y\": y}).to_csv(filename, index=False)\n",
    "        logger.info(f\"Data generated at {filename.absolute()}.\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = Path(\"../data/sim\")\n",
    "if not datadir.exists():\n",
    "    datadir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_a = 2\n",
    "var_b = 4\n",
    "filename = datadir / f\"linear_a{var_a}_b{var_b}.csv\"\n",
    "make_linear(filename, a=var_a, b=var_b)\n",
    "df = pd.read_csv(filename)\n",
    "plt.plot(df[\"x\"], df[\"y\"], \"b+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for a line is:\n",
    "\n",
    "$$model(x)= a * x + b$$\n",
    "\n",
    "We have set the slope $a=2$ and the intercept $b=4$.\n",
    "\n",
    "## 1.2 Statistical modelling\n",
    "\n",
    "It's obvisous that there is a lot of noise, but let's try to fit a linear model.\n",
    "\n",
    "We will use pymc for the sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will assume that both $a$ and $b$ follow a normal distribution.\n",
    "We can write this down with this notation:\n",
    "\n",
    "$$a \\sim \\mathcal{N}(\\mu, \\sigma)$$\n",
    "$$b \\sim \\mathcal{N}(\\mu, \\sigma)$$\n",
    "\n",
    "This means: we model $a$ and $b$ as normally distributed with mean $\\mu$ and standard deviation $\\sigma$, even if we dont know (yet) what those exact parameters are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename)\n",
    "x = df[\"x\"]\n",
    "y = df[\"y\"]\n",
    "\n",
    "model = pm.Model()\n",
    "with model:\n",
    "    a = pm.Normal(\"a\")\n",
    "    b = pm.Normal(\"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see below, the model sort of what we expected. You can also see that we start with $\\mu=0$ and $\\sigma=1$, these are just starting points (we have to start somewhere) and this will converge when we look at data. If you are wondering \"yeah but what if I have some vague idea about the mean value and want to model that too\" you are on the right track! For now, let's keep it simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We will assume that our model is not perfect. There will be noise.:\n",
    "\n",
    "$$model(x) = a * x + b + noise$$\n",
    "\n",
    "But we can assume the noise itself will also follow a normal distribution, with a mean $\\mu$ and a standard deviation $\\sigma$:\n",
    "$$ noise \\sim \\mathscr{N}(\\mu, \\sigma)$$\n",
    "\n",
    "This means there are 4 parameters we want to figure out:\n",
    " - slope $a$\n",
    " - intercept $b$\n",
    " - mean $\\mu$ from the noise\n",
    " - standard deviation $\\sigma$ from the noise\n",
    "\n",
    "We should be able to calculate the $\\mu$, because we expect:\n",
    " $$model(x) - (a*x + b) = \\mathscr{N}(0, \\sigma)$$\n",
    "\n",
    " which is the same as:\n",
    "\n",
    " $$model(x) = \\mathscr{N}(a*x+b, \\sigma)$$\n",
    "or, more general,\n",
    " $$model(x) = \\mathscr{N}(f_{\\theta}(x), \\sigma)$$\n",
    "\n",
    "Where $f$ is some function, and $\\theta$ are the learnable parameters of the model. In this case, $\\theta = (a, b)$.\n",
    "\n",
    "At first, this might seem confusing. Why is the mean suddenly a function? Shouldn't the mean be a fixed number? \n",
    "\n",
    "What we are actually saying is this: lets imagine you want to sample our model for a given x, eg `x=0.4`. Because our model is probabilistic, we don't expect to get the same values every time. The results are a random process. If you would sample the model with `x=0.4` a 100 times, we assume output would follow a distribution. What sort of distribution? Well, in this model, a normal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "noise = y.values - (x.values * var_a + var_b)\n",
    "ax[0].plot(x.values, y.values, \"b+\")\n",
    "ax[0].set_title(\"Values with linear trend\")\n",
    "ax[1].plot(x.values, noise, \"b+\")\n",
    "ax[1].set_title(\"Residual noise: Values with linear subtracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If I take the observed value $y$ and subtract the model $ax + b$, I keep the noise and that noise will have a mean of 0 and an unknown standard deviation of $\\sigma$. If I dont subtract the model, I no longer have a mean of zero, but a mean of $ax + b$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.3 How to model the standard deviation?\n",
    " Because a standard deviation can't be negative, we could run into problems if we model it as a normal distribution. We run the chance of sampling negative values, which will cause errors. So, we will need a distribution that is strictly positive.\n",
    "\n",
    " We can use a halfnormal distribution for this, which is just a normal distribution, but one that cant become negative. There are better options for modeling standard deviations, but let's start with this for now.\n",
    "\n",
    " the `with` syntax allows us to extend our existing model. We can add new variables to the model, and we can also add new observations to the model. This is a very powerful feature of pymc3, and it allows us to build complex models in a very readable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    sigma = pm.HalfNormal(\"sigma\")\n",
    "    predict = a * x + b\n",
    "    estimate = pm.Normal(\"y\", mu=predict, sigma=sigma, observed=y)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we expanded our model. We now have some sort of \"causal chain\" of variables.\n",
    "We start with $a$ and $b$ and $sigma$, and those are combined into our model $(a*x + b)$, and the output of the this prediction is taken to be the mean of the actual value plus noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Sampling\n",
    "With this model, we can run simulations to figure out the best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    result = pm.sample(3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note pm uses 4 chains. This is similar to what with did with estimating $\\pi$: every chain has its own randomness, but combining 4 chains and taking the average of every chain reduces the randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "az.plot_trace(result)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a really nice answer!\n",
    "\n",
    "Because we generated the data for ourselves, we actually know that the true answers are $a=2.0$, $b=4.0$ and $\\sigma=0.5$.\n",
    "\n",
    "All the answers are pretty close! And we also have a good grip on how certain we are, given the limited amount of data. \n",
    "\n",
    "Using more data would decrease our uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modeling more complex functions: Sinewave\n",
    "\n",
    "## 2.1 Generating synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the formula of the model follows the same template. I will demonstrate the same pattern, but this time with a sine wave.\n",
    "\n",
    "Let's make a sine wave with an amplitude of 3, frequency of 2, and some noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sine_wave(\n",
    "    filename: Path,\n",
    "    a: float,\n",
    "    f: float,\n",
    "    s: float,\n",
    "    t: np.ndarray = np.linspace(0, 4, 100),\n",
    ") -> None:\n",
    "    if filename.exists():\n",
    "        logger.info(f\"Data already generated in {filename.absolute()}\")\n",
    "    else:\n",
    "        noise = np.random.normal(scale=s, size=len(t))\n",
    "        v = a * np.sin(f * t) + noise\n",
    "        pd.DataFrame({\"time\": t, \"value\": v}).to_csv(filename, index=False)\n",
    "        logger.info(f\"Data generated at {filename.absolute()}.\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_a = 3\n",
    "var_f = 2\n",
    "var_s = 0.2\n",
    "filename = datadir / f\"sine_a{var_a}_f{var_f}_s{var_s}.csv\"\n",
    "make_sine_wave(filename, a=var_a, f=var_f, s=var_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename)\n",
    "plt.plot(df[\"time\"], df[\"value\"], \"b+\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"value\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Weakly informative distributions\n",
    "As you can see, there is some noise, but not too much.\n",
    "We got good results with the halfnorm distribution, but we can do better.\n",
    "\n",
    "As a modeller, it is our task to express our knowledge about the situation in assumptions. We try to stay on the safe side, but adding a little bit of knowledge to our model is a good idea.\n",
    "\n",
    "Because, saying we know absolutely nothing more about the standard deviation than that it is positive is a bit too strong.\n",
    "\n",
    "We could speed up the calculations by using [what is called a \"weakly informative\" distribution.](http://www.stat.columbia.edu/~gelman/research/published/taumain.pdf)\n",
    "\n",
    "Basically, we want to add just a little bit more of knowledge to our beliefs about the standard deviation, but not too much.\n",
    "\n",
    "The prior should be picked, such that the information it does provide is intentionally weaker than whatever actual knowledge is available, just to be on the safe side.\n",
    "\n",
    "For example, we can be sure the standard deviaition is below 100. Also, it expect it to be somewhat above 0.\n",
    "\n",
    "The paper linked suggests using an inverse gamma.\n",
    "Lets plot the different options, comparing some distributions, to find out if we can better understand why that makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "x = df[\"time\"]\n",
    "ax[0].plot(x, stats.halfnorm.pdf(x))\n",
    "ax[0].set_title(\"Halfnorm\")\n",
    "\n",
    "ax[1].plot(x, stats.halfcauchy.pdf(x))\n",
    "ax[1].set_title(\"Half Cauchy\")\n",
    "\n",
    "ax[2].plot(x, stats.invgamma.pdf(x, a=1))\n",
    "ax[2].set_title(\"Inverse Gamma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To me, the inverse gamma indeed looks like what I would expect. Not zero, very high is also unlikely, and somewere between 0.1 and 3 seems very reasonable as a guess.\n",
    "\n",
    "If you are afraid of picking the wrong distribution, no worries! As you have seen before, a Halfnormal will work just fine. Even starting with a uniform distribution will work, even though it will probably be less efficient.\n",
    "The Half Cauchy will also work fine in this case, as will the Inverse Gamma. However, if your data becomes more complex, picking the right balance between more neutral distributions (eg uniform) and more informed distributions, like the Inverse Gamma, could make a difference.\n",
    "\n",
    "In general, because it is reasonable to assume that the standard deviation will neither be 0, nor very high, the inverse gamma will converge faster in most cases.\n",
    "\n",
    "## 2.3 Sine wave model\n",
    "\n",
    "In this case, our model looks like this:\n",
    "\n",
    "$$Amp \\sim \\mathcal{N}(0, 1)$$\n",
    "$$freq \\sim \\mathcal{N}(0, 1)$$\n",
    "$$predict \\sim Amp * sin(freq * x)$$\n",
    "$$\\sigma \\sim InverseGamma(a=1)$$\n",
    "$$y \\sim \\mathcal{N}(predict, \\sigma)$$\n",
    "\n",
    "\n",
    "If this is confusing, try to read it from the bottom up: we model the residual as a normal distribution. The residual will have our predcitive model as the mean, and there will be some uncertainty in our model that is described by the standard deviation. Higher up in the \"causal chain\" we find the standard deviation $\\sigma$, which we are uncertain about and model with an InverseGamma, and also parameters of the model that we want to find out (like the amplitude and the frequency) and model with a normal distribution.\n",
    "\n",
    "Sometimes people draw diagrams with arrows to make the causal route in their probabilistic models more explicit.\n",
    "\n",
    "## 2.4 Sampling\n",
    "Now lets implement the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "\n",
    "df = pd.read_csv(filename)\n",
    "x = df[\"time\"]\n",
    "y = df[\"value\"]\n",
    "\n",
    "model = pm.Model()\n",
    "\n",
    "with model:\n",
    "    Amplitude = pm.Normal(\"amp\")\n",
    "    freq = pm.Normal(\"f\")\n",
    "    predict = Amplitude * np.sin(freq * x)\n",
    "\n",
    "    sigma = pm.InverseGamma(\"sigma\", alpha=1)\n",
    "    estimate = pm.Normal(\"y\", mu=predict, sigma=sigma, observed=y)\n",
    "\n",
    "    result = pm.sample(3000)\n",
    "az.plot_trace(result)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Improving the model\n",
    "Sometimes the amplitude and frequency have both negative and positive values. This is caused by the way the sine wave formula works. \n",
    "Both actually give the same result, but the sampler might inform you that it wasn't able to converge properly. \n",
    "\n",
    "If you dont like that, you could force the amplitude and frequency to be positive, for example with a HalfNormal distribution (that only allows positive values), or you could also use a uniform distribution with a lower bound of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pm.Model()\n",
    "\n",
    "with model:\n",
    "    Amplitude = pm.HalfNormal(\"amp\")\n",
    "    freq = pm.HalfNormal(\"f\")\n",
    "    predict = Amplitude * np.sin(freq * x)\n",
    "\n",
    "    sigma = pm.InverseGamma(\"sigma\", alpha=1)\n",
    "    estimate = pm.Normal(\"y\", mu=predict, sigma=sigma, observed=y)\n",
    "\n",
    "    result = pm.sample(3000)\n",
    "az.plot_trace(result)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Plotting the results\n",
    "We can plot the final result, adding two standard deviations as upper and lower bounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = result.posterior.amp.median().values\n",
    "f = result.posterior.f.median().values\n",
    "sigma = result.posterior.sigma.median().values\n",
    "yhat = A * np.sin(f * df[\"time\"])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df[\"time\"], df[\"value\"], \"b+\")\n",
    "plt.plot(df[\"time\"], yhat, color=\"black\", label=\"model\")\n",
    "plt.fill_between(df[\"time\"], yhat + sigma, yhat - sigma, color=\"r\", alpha=0.5)\n",
    "plt.fill_between(df[\"time\"], yhat + 2 * sigma, yhat - 2 * sigma, color=\"r\", alpha=0.3);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
