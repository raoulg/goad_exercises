{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements the Thompson sampling algorithm.\n",
    "First lets look at the pseudo code for the algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ab_thompson.png\" alt=\"thompson algorithm\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beta distributions\n",
    "Beta distributions have the useful property that the parameters a and b can be interpreted as cointosses:\n",
    "- a is the number of wins\n",
    "- b is the number of losses\n",
    "\n",
    "So let's say we have a \"true\" coin that has a 40% chance of winning. This \"true\" chance $p$ is fixed, but our belief over that chance will evolve over time when we gather more observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 100)\n",
    "plt.plot(x, stats.beta(4, 6).pdf(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see the beta distribution after 40 wins, 60 losses. Play around with the parameters to see how the distribution changes.\n",
    "\n",
    "# Multi-armed bandits\n",
    "\n",
    "These are imaginary gambling machines with multiple arms. The question is: which arm should we pull to maximize our winnings? The problem is that we don't know the probability of winning for each arm. So we have to explore the arms to find out which one is the best. But we also want to exploit the best arm to maximize our winnings. This is the explore-exploit dilemma.\n",
    "\n",
    "This is a very typical real life scenario. Let's say you have a product, and want to know what the optimal price is for selling the product. Making it cheaper might drive up the price, but you also need to sell more before you get the same profit. Now let's say you are a non-baysian data scientist and you tell the business:\n",
    "\n",
    "\"This is a simple problem. We just need to offer the bike a 1000 times for the low price, and a 1000 times for the high price. Afterwards I can tell you what the best price is.\" \n",
    "\n",
    "Regardless of which of the two prices is the best, the business probably wont be happy with the result: it has cost them a lot of money to find out what the best price is! And a 1000 potential clients have been offered the suboptimal price! Can't we do better than this?\n",
    "\n",
    "It turns out, yes! We will use bayesian statistics to improve our method and implement the Thompsons sampling algorithm.\n",
    "\n",
    "# priors\n",
    "We need beta priors to define our prior belief about the situation. It is very rare that you have really no idea at all. You might ask the business: can you guess, out of 10 clients, how many of them will buy the bike for price a? If you get an answer, this is your prior!\n",
    "Selling a bike is a win, so 3 out of 10 should be translated into $Beta(3, 7)$. Note that this is NOT the same as $Beta(30, 70)$, which is a much stronger prior. The first prior is very weak, the second is very strong. The first prior will be easily overruled by the data, the second prior will be much harder to overrule with new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BetaPrior:\n",
    "    a: int\n",
    "    b: int\n",
    "    p: float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we dont have a \"real world\" to get information from, we will need to define the \"real\" percentage. Normally, you dont have access to this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = [BetaPrior(3, 7, 0.30), BetaPrior(3, 7, 0.35)]\n",
    "priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to pull the \"arms\" of the \"multi armed bandit\". Lets create actual beta distributions with our prior beliefs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arms = [stats.beta(p.a, p.b) for p in priors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lets sample a single value from each of these distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [bandit.rvs() for bandit in arms]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should expect a value close to 30% for every arm. Note that this sampling adds \"exploration\" to the algorithm. We are not just simply (and greedy) exploiting what seems to be the best arm, because we might be fooled by (un)lucky samples! If the distributions are close, we might randomly get the one or the other every time. \n",
    "\n",
    "So let's take the best one of these two samples (which involved a bit of luck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm = np.argmax(sample)\n",
    "arm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is the arm that had the luck to be picked for this round. Now lets actually pull the arm with the \"real\" p value, and see if we actually win or lose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_p = priors[arm].p\n",
    "stats.bernoulli(real_p).rvs(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I pulled it 10 times, so you should expect to see about 3 or 4 wins.\n",
    "Let's simulation this \"real world reward\" in a separate class, that can \"pay out\" rewards after picking the number of an arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reward:\n",
    "    def __init__(self, priors: list[BetaPrior]) -> None:\n",
    "        self.p = [beta.p for beta in priors]\n",
    "        self.bernoulli = [stats.bernoulli(beta.p) for beta in priors]\n",
    "\n",
    "    def pay(self, arm: int) -> bool:\n",
    "        return bool(self.bernoulli[arm].rvs())\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Reward({self.p})\"\n",
    "\n",
    "\n",
    "reward = Reward(priors)\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward.pay(arm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "every time we win, we can update our beliefs by adding a \"win\" to the a parameter. If we loose, we will add a 1 to the b parameter. This is the \"learning\" part of the algorithm. We are updating our beliefs based on the data we are gathering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all these ingredients, we can wrap them all into a single class that can simulate the multi-armed bandit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit:\n",
    "    def __init__(self, priors: list[BetaPrior]):\n",
    "        self.arms = [stats.beta(p.a, p.b) for p in priors]\n",
    "        self.rewards = Reward(priors)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        args = [arm.args for arm in self.arms]\n",
    "        return f\"Bandit({args})\"\n",
    "\n",
    "    def pull(self) -> None:\n",
    "        sample = [arm.rvs() for arm in self.arms]\n",
    "        # the \"luck\" part of sampling helps us still explore other arms a bit\n",
    "        arm = np.argmax(sample)\n",
    "        reward = self.rewards.pay(arm)\n",
    "\n",
    "        a, b = self.arms[arm].args\n",
    "        if reward:\n",
    "            # winning\n",
    "            self.arms[arm].args = (a + 1, b)\n",
    "        else:\n",
    "            # loosing\n",
    "            self.arms[arm].args = (a, b + 1)\n",
    "\n",
    "    def plot(self, ax=None) -> None:\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "        x = np.linspace(0, 1, 100)\n",
    "        for i in range(len(self.arms)):\n",
    "            ax.plot(x, self.arms[i].pdf(x), label=f\"Bandit {i}\")\n",
    "        ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = [BetaPrior(3, 7, p=0.30), BetaPrior(4, 7, p=0.35)]\n",
    "multiarmedbandit = MultiArmedBandit(priors)\n",
    "multiarmedbandit.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I changed the priors a bit, so you can have an idea how the two different priors influence the starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# priors = [BetaPrior(23,27, p=0.45) , BetaPrior(27, 23, p=0.55), BetaPrior(30, 20, p=0.60)]\n",
    "priors = [BetaPrior(2, 2, p=0.25), BetaPrior(2, 2, p=0.3), BetaPrior(2, 2, p=0.35)]\n",
    "multiarmedbandit = MultiArmedBandit(priors)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(9):\n",
    "    for _ in range(50):\n",
    "        multiarmedbandit.pull()\n",
    "    multiarmedbandit.plot(ax=axes[i])\n",
    "    axes[i].set_title(f\"{str(multiarmedbandit)}\")\n",
    "plt.tight_layout()\n",
    "\n",
    "total1 = sum(multiarmedbandit.arms[0].args)\n",
    "total2 = sum(multiarmedbandit.arms[1].args)\n",
    "total3 = sum(multiarmedbandit.arms[2].args)\n",
    "total = total1 + total2 + total3\n",
    "\n",
    "print(\n",
    "    f\"Arm 1 has been pulled {total1} times, {100 * total1 / (total):.2f}% of {total}\\n\"\n",
    "    f\"Arm 2 has been pulled {total2} times, {100 * total2 / (total):.2f}% of {total}\\n\"\n",
    "    f\"Arm 3 has been pulled {total3} times, {100 * total3 / (total):.2f}% of {total}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this experiment, you will see (most of the times! It is still a random method that involves chance...) that bandit 2 with the best percentage will be preferred over the others, typically by a lot!\n",
    "\n",
    "This is a very elegant way to balance statistical rigor with business needs. We are not just randomly exploring, we are exploring in a smart way, and exploit where that seems justified. This will maximize our winnings considerably over time!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course-materials-uos1-KNtHm8fd-mads-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
